{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c22201cf-5581-4bff-aed6-67cdfe4feed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Applying BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d57b3614-90e5-45c9-a6d2-e8a9f9fb30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv('Suicide_Detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a568b508-4afd-40c6-ba28-01d3ca0fae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp_df.iloc[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166a1b74-347e-468d-8bcd-7ff6bd2d4f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Iâ€™m so lostHello, my name is Adam (16) and Iâ€™v...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class\n",
       "0           2  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "1           3  Am I weird I don't get affected by compliments...  non-suicide\n",
       "2           4  Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "3           8          i need helpjust help me im crying so hard      suicide\n",
       "4           9  Iâ€™m so lostHello, my name is Adam (16) and Iâ€™v...      suicide"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cc07d8-9792-44d8-9f4c-ee7528441ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ulka\\AppData\\Local\\Temp\\ipykernel_4548\\1160589198.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(['Unnamed: 0'],axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 0'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60618bae-9b18-43bc-a0a6-686b781824ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iâ€™m so lostHello, my name is Adam (16) and Iâ€™v...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "1  Am I weird I don't get affected by compliments...  non-suicide\n",
       "2  Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "3          i need helpjust help me im crying so hard      suicide\n",
       "4  Iâ€™m so lostHello, my name is Adam (16) and Iâ€™v...      suicide"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b8e4d5-f45d-46ce-87a9-2450782cc551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ulka\\AppData\\Local\\Temp\\ipykernel_4548\\3534604001.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x : x.lower())\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f119d1-75c0-47ef-9a7c-db28a1bc74bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ulka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ulka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary resources from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8a1868-000c-44e4-8221-006bb3c52f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ulka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ulka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Download NLTK POS tagger and tokenizer if you haven't\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ca6d3d-94b1-47a3-bed4-55dc45b4f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Define POS tags that are important for depression detection\n",
    "# IMPORTANT_POS = {\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"PRP\", \"PRP$\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}\n",
    "\n",
    "# def filter_important_pos_nltk(text):\n",
    "#     # Tokenize the text\n",
    "#     tokens = word_tokenize(text)\n",
    "    \n",
    "#     # Get POS tags for each token\n",
    "#     tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "#     # Keep only tokens that have important POS tags\n",
    "#     filtered_tokens = [word for word, pos in tagged_tokens if pos in IMPORTANT_POS]\n",
    "    \n",
    "#     # Return filtered tokens as a string or a list\n",
    "#     return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "# df['text'] = df['text'].apply(\n",
    "#     lambda x: filter_important_pos_nltk(x)\n",
    "# )\n",
    "\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9d588f-9d73-40c4-bea1-2d9aea45612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ulka\\AppData\\Local\\Temp\\ipykernel_4548\\2078368733.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ex, wife, threatening, suiciderecently, left,...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[weird, dont, get, affected, compliments, comi...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[finally, almost, never, hear, bad, year, ever...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[need, helpjust, help, im, crying, hard]</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[losthello, name, adam, struggling, years, afr...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "0  [ex, wife, threatening, suiciderecently, left,...      suicide\n",
       "1  [weird, dont, get, affected, compliments, comi...  non-suicide\n",
       "2  [finally, almost, never, hear, bad, year, ever...  non-suicide\n",
       "3           [need, helpjust, help, im, crying, hard]      suicide\n",
       "4  [losthello, name, adam, struggling, years, afr...      suicide"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Additional cleaning: remove non-alphabetic tokens and convert to lowercase\n",
    "def clean_tokens(tokens):\n",
    "    return [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "# Apply all steps: remove punctuation, tokenize, clean tokens, and remove stop words\n",
    "df['text'] = df['text'].apply(\n",
    "    lambda x: clean_tokens(remove_stopwords(tokenize(remove_punctuation(x))))\n",
    ")\n",
    "\n",
    "# Output the cleaned and tokenized DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748364ae-0772-42c4-95cf-94d9ab98d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ulka\\AppData\\Local\\Temp\\ipykernel_4548\\847003611.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(stem_tokens)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ex, wife, threaten, suiciderec, left, wife, g...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[weird, dont, get, affect, compliment, come, s...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[final, almost, never, hear, bad, year, ever, ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[need, helpjust, help, im, cri, hard]</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[losthello, name, adam, struggl, year, afraid,...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "0  [ex, wife, threaten, suiciderec, left, wife, g...      suicide\n",
       "1  [weird, dont, get, affect, compliment, come, s...  non-suicide\n",
       "2  [final, almost, never, hear, bad, year, ever, ...  non-suicide\n",
       "3              [need, helpjust, help, im, cri, hard]      suicide\n",
       "4  [losthello, name, adam, struggl, year, afraid,...      suicide"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to apply stemming\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Apply the cleaning functions and stemming to the 'tokenized_review' column\n",
    "df['text'] = df['text'].apply(stem_tokens)\n",
    "\n",
    "# Output the cleaned and stemmed DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "319dd2ed-3458-4f8c-86cf-c46041bafd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ulka\\AppData\\Local\\Temp\\ipykernel_4548\\681359426.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda tokens: ' '.join(tokens))\n"
     ]
    }
   ],
   "source": [
    "# Join the tokenized lists into strings for CountVectorizer\n",
    "df['text'] = df['text'].apply(lambda tokens: ' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "970aba32-7a2b-4521-86b1-4ac0cdfcf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:1]\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1842b837-d220-414a-abfe-63160e317f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f018e5d-0df4-4a4e-b382-38193176cb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#suicide=1, non-suicide=0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3e3976-efd9-4b3e-a255-5b2c5cbfdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "050bf3ae-7e12-4c97-9007-2505377ff975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "819a7110-281f-49fe-b487-cca782086b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unigrams and bigrams\n",
    "cv2 = CountVectorizer(ngram_range=(1, 2), max_features=2000)  # This includes both unigrams and bigrams  #20000 overall accuracy increased\n",
    "X_train_bow2 = cv2.fit_transform(X_train['text']).toarray()\n",
    "X_test_bow2 = cv2.transform(X_test['text']).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd8fd31e-348e-41d0-b630-2c7bd5cf3f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8695"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow2,y_train)\n",
    "y_pred = rf.predict(X_test_bow2)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b51f0019-8465-490e-a778-96c66b1beda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1733,  304],\n",
       "       [ 218, 1745]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "094abc22-5578-4886-8dca-333a6f6fc155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "Precision: 0.85\n",
      "Recall: 0.89\n",
      "F1 Score: 0.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87      2037\n",
      "           1       0.85      0.89      0.87      1963\n",
      "\n",
      "    accuracy                           0.87      4000\n",
      "   macro avg       0.87      0.87      0.87      4000\n",
      "weighted avg       0.87      0.87      0.87      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1162ad45-7858-4a36-8743-2e35807e5b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sure here upbeat posit whatsappstyl convers hey guess friend hey happen got job offer wait friend omg amaz congratul knew get thank excit believ final happen friend total deserv hard work final paid start two week alreadi sign offer letter got ta wrap thing current job friend great need celebr dinner weekend definit let still shock haha friend happi begin go crush new job thank bit nervou mostli excit start someth new go big chang friend great chang trust go awesom proud best wait dinner weekend convers fill excit posit around great achiev\n",
      "[0]\n",
      "Text: \"sure here upbeat posit whatsappstyl convers hey guess friend hey happen got job offer wait friend omg amaz congratul knew get thank excit believ final happen friend total deserv hard work final paid start two week alreadi sign offer letter got ta wrap thing current job friend great need celebr dinner weekend definit let still shock haha friend happi begin go crush new job thank bit nervou mostli excit start someth new go big chang friend great chang trust go awesom proud best wait dinner weekend convers fill excit posit around great achiev\" - Predicted Label: non-suicide, Probability of Suicide: 0.3700\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "# # Define POS tags that are important for depression detection\n",
    "# IMPORTANT_POS = {\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"PRP\", \"PRP$\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}\n",
    "\n",
    "# def filter_important_pos_nltk(text):\n",
    "#     # Tokenize the text\n",
    "#     tokens = word_tokenize(text)\n",
    "    \n",
    "#     # Get POS tags for each token\n",
    "#     tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "#     # Keep only tokens that have important POS tags\n",
    "#     filtered_tokens = [word for word, pos in tagged_tokens if pos in IMPORTANT_POS]\n",
    "    \n",
    "#     # Return filtered tokens as a string or a list\n",
    "#     return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess a single text entry\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and apply stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [stemmer.stem(word.lower()) for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the cleaned tokens into a single string\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Example usage with a new text entry\n",
    "new_data = '''Sure! Here's a more upbeat and positive WhatsApp-style conversation:\n",
    "\n",
    "---\n",
    "\n",
    "**[10:15 AM]**\n",
    "*You*: Hey! Guess what!\n",
    "\n",
    "**[10:17 AM]**\n",
    "*Friend*: Hey! Whatâ€™s up? What happened?\n",
    "\n",
    "**[10:18 AM]**\n",
    "*You*: I just got the job offer Iâ€™ve been waiting for! ðŸŽ‰\n",
    "\n",
    "**[10:19 AM]**\n",
    "*Friend*: OMG! Thatâ€™s amazing! Congratulations!! ðŸ¥³ I knew youâ€™d get it!!\n",
    "\n",
    "**[10:20 AM]**\n",
    "*You*: Thanks! Iâ€™m so excited, I canâ€™t believe it finally happened! ðŸ˜„\n",
    "\n",
    "**[10:21 AM]**\n",
    "*Friend*: You totally deserve it! All your hard work finally paid off. So when do you start?\n",
    "\n",
    "**[10:22 AM]**\n",
    "*You*: In two weeks! I already signed the offer letter. Just gotta wrap up some things at my current job.\n",
    "\n",
    "**[10:23 AM]**\n",
    "*Friend*: Thatâ€™s so great! We need to celebrate! ðŸŽ‰ Dinner this weekend?\n",
    "\n",
    "**[10:24 AM]**\n",
    "*You*: Definitely! Letâ€™s do it! Iâ€™m still in shock, haha.\n",
    "\n",
    "**[10:25 AM]**\n",
    "*Friend*: Iâ€™m so happy for you!! This is just the beginning. Youâ€™re going to crush it at your new job!\n",
    "\n",
    "**[10:27 AM]**\n",
    "*You*: Thanks! Iâ€™m a bit nervous but mostly just excited to start something new. Itâ€™s going to be such a big change.\n",
    "\n",
    "**[10:28 AM]**\n",
    "*Friend*: Itâ€™ll be a great change, trust me. Youâ€™re going to do awesome! So proud of you! â¤ï¸\n",
    "\n",
    "**[10:29 AM]**\n",
    "*You*: Youâ€™re the best! Canâ€™t wait for dinner this weekend!\n",
    "\n",
    "---\n",
    "\n",
    "This conversation is filled with excitement and positivity around a great achievement!'''.lower()\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data = preprocess_text(new_data)\n",
    "\n",
    "# Output the cleaned text\n",
    "print(new_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame for the new data\n",
    "data1 = pd.DataFrame({'text': [new_data]})\n",
    "\n",
    "# Transform the new data using the fitted vectorizer\n",
    "X1 = cv2.transform(data1['text']).toarray()  # Correctly access the 'text' column\n",
    "X1.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the new data\n",
    "new_predictions = rf.predict(X1)\n",
    "print(new_predictions)\n",
    "\n",
    "# Make predictions on the new data\n",
    "new_probabilities = rf.predict_proba(X1)\n",
    "\n",
    "# Display the results\n",
    "for text, probabilities in zip(data1['text'], new_probabilities):\n",
    "    # Extract the probability of the positive class (class 1)\n",
    "    probability_suicide = probabilities[1]  # Probability of class 1 (suicide)\n",
    "    label = \"suicide\" if probability_suicide >= 0.5 else \"non-suicide\"\n",
    "    \n",
    "    # Print text with predicted label and probability\n",
    "    print(f'Text: \"{text}\" - Predicted Label: {label}, Probability of Suicide: {probability_suicide:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Display the results\n",
    "# for text, prediction in zip(data1['text'], new_predictions):\n",
    "#     label = \"suiside\" if prediction == 1 else \"non-suicide\"\n",
    "#     print(f'Text: \"{text}\" - Predicted Label: {label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cb02f8c-bf12-4f59-868d-cfe1225bba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the model\n",
    "# with open('model.pkl', 'wb') as model_file:\n",
    "#     pickle.dump(rf, model_file)\n",
    "\n",
    "# # Save the vectorizer\n",
    "# with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "#     pickle.dump(cv2, vectorizer_file)  # Assuming 'cv2' is your vectorizer (CountVectorizer or TfidfVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632c1be-f95f-4ff8-b256-2551eb140022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bae61-a417-4e48-add5-b814db5a4a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
